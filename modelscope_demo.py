#!/usr/bin/env python3
"""
ModelScope å¤šæ¨¡æ€åŠŸèƒ½æ¼”ç¤ºä»£ç 
å±•ç¤ºæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ç­‰ä¸»æµAIä»»åŠ¡
"""

import torch
import numpy as np
from PIL import Image
import soundfile as sf
import matplotlib.pyplot as plt
from modelscope import snapshot_download
from modelscope.pipelines import pipeline
from modelscope.models import Model
from modelscope.preprocessors import Preprocessor
from modelscope.msdatasets import MsDataset
from modelscope.trainers import build_trainer
from modelscope.outputs import OutputKeys
import warnings

warnings.filterwarnings('ignore')


def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("=" * 60)
    print("ModelScope åŠŸèƒ½æ¼”ç¤º")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    print("=" * 60 + "\n")


def download_model_example():
    """ç¤ºä¾‹1ï¼šä¸‹è½½æ¨¡å‹"""
    print("ğŸ“¥ 1. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹")

    model_id = 'damo/nlp_structbert_sentence-similarity_chinese-base'
    try:
        model_dir = snapshot_download(
            model_id=model_id,
            cache_dir='./models',  # æŒ‡å®šç¼“å­˜ç›®å½•
            revision='v1.0.0'  # æŒ‡å®šç‰ˆæœ¬
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
        return model_dir
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None


def nlp_text_classification():
    """ç¤ºä¾‹2ï¼šNLPæ–‡æœ¬åˆ†ç±»"""
    print("\nğŸ“ 2. NLPæ–‡æœ¬åˆ†ç±»")

    # åˆ›å»ºæ–‡æœ¬åˆ†ç±»pipeline
    classifier = pipeline(
        task='text-classification',
        model='damo/nlp_structbert_classification_chinese-base'
    )

    texts = [
        "è¿™éƒ¨ç”µå½±çœŸæ˜¯å¤ªç²¾å½©äº†ï¼Œæ¼”å‘˜æ¼”æŠ€åœ¨çº¿ï¼",
        "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œäº§å“è´¨é‡ä¹Ÿæœ‰é—®é¢˜ã€‚",
        "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚"
    ]

    for text in texts:
        result = classifier(text)
        print(f"\næ–‡æœ¬: {text[:30]}...")
        print(f"æƒ…æ„Ÿ: {result['label']} (ç½®ä¿¡åº¦: {result['score']:.3f})")

    return classifier


def nlp_sentence_similarity():
    """ç¤ºä¾‹3ï¼šå¥å­ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\nğŸ”— 3. å¥å­ç›¸ä¼¼åº¦è®¡ç®—")

    sim_pipeline = pipeline(
        task='sentence-similarity',
        model='damo/nlp_structbert_sentence-similarity_chinese-base'
    )

    sentences = [
        "æˆ‘å–œæ¬¢åƒè‹¹æœ",
        "è‹¹æœæ˜¯ä¸€ç§æ°´æœ",
        "æˆ‘æ­£åœ¨ä½¿ç”¨è‹¹æœæ‰‹æœº",
        "é¦™è•‰æ˜¯é»„è‰²çš„æ°´æœ"
    ]

    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    results = sim_pipeline(input=(sentences, sentences))

    print("\nå¥å­ç›¸ä¼¼åº¦çŸ©é˜µ:")
    for i, sent1 in enumerate(sentences):
        similarities = []
        for j, sent2 in enumerate(sentences):
            similarity = results[i][j]
            similarities.append(f"{similarity:.3f}")
        print(f"{sent1[:15]:<15} | {' '.join(similarities)}")


def computer_vision_segmentation():
    """ç¤ºä¾‹4ï¼šè®¡ç®—æœºè§†è§‰ - å›¾åƒåˆ†å‰²"""
    print("\nğŸ–¼ï¸  4. å›¾åƒåˆ†å‰² (äººåƒæŠ å›¾)")

    try:
        # åˆ›å»ºå›¾åƒåˆ†å‰²pipeline
        segmenter = pipeline(
            task='image-matting',
            model='damo/cv_unet_image-matting'
        )

        # ç¤ºä¾‹ï¼šå¦‚æœæ²¡æœ‰å®é™…å›¾ç‰‡ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿå›¾ç‰‡
        print("âš ï¸  ç”±äºæ²¡æœ‰çœŸå®å›¾ç‰‡ï¼Œè¿™é‡Œå±•ç¤ºè°ƒç”¨æ–¹å¼")
        print("å®é™…ä½¿ç”¨æ—¶ï¼Œä¼ å…¥å›¾ç‰‡è·¯å¾„å³å¯:")
        print("result = segmenter('path/to/your/image.jpg')")
        print(f"è¾“å‡ºåŒ…å«: {OutputKeys.OUTPUT_IMG}")

        # å¦‚æœçœŸçš„æœ‰å›¾ç‰‡ï¼Œå¯ä»¥è¿™æ ·ç”¨ï¼š
        # result = segmenter({
        #     'image': 'input.jpg',
        #     'background': 'background.jpg'  # å¯é€‰ï¼Œæ›¿æ¢èƒŒæ™¯
        # })
        # Image.fromarray(result[OutputKeys.OUTPUT_IMG]).save('output.png')

        return segmenter
    except Exception as e:
        print(f"âŒ å›¾åƒåˆ†å‰²åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def speech_recognition_example():
    """ç¤ºä¾‹5ï¼šè¯­éŸ³è¯†åˆ«"""
    print("\nğŸ¤ 5. è¯­éŸ³è¯†åˆ« (ASR)")

    try:
        # åˆ›å»ºè¯­éŸ³è¯†åˆ«pipeline
        asr_pipeline = pipeline(
            task='auto-speech-recognition',
            model='damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'
        )

        print("æ”¯æŒçš„éŸ³é¢‘æ ¼å¼: WAV, MP3, FLACç­‰")
        print("é‡‡æ ·ç‡è¦æ±‚: 16kHz")
        print("\nå®é™…è°ƒç”¨æ–¹å¼:")
        print("result = asr_pipeline('audio.wav')")
        print("text = result['text']  # è¯†åˆ«ç»“æœ")

        # å¦‚æœæœ‰éŸ³é¢‘æ–‡ä»¶ï¼Œå¯ä»¥è¿™æ ·ç”¨ï¼š
        # result = asr_pipeline('path/to/audio.wav')
        # print(f"è¯†åˆ«ç»“æœ: {result['text']}")

        return asr_pipeline
    except Exception as e:
        print(f"âŒ è¯­éŸ³è¯†åˆ«åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


def dataset_loading_example():
    """ç¤ºä¾‹6ï¼šåŠ è½½æ•°æ®é›†"""
    print("\nğŸ“Š 6. åŠ è½½æ•°æ®é›†")

    try:
        # åŠ è½½ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†
        dataset = MsDataset.load(
            'afqmc',
            namespace='modelscope',
            subset_name='default',
            split='train'
        )

        print(f"æ•°æ®é›†: AFQMC (èš‚èšé‡‘èè¯­ä¹‰ç›¸ä¼¼åº¦)")
        print(f"æ ·æœ¬æ•°: ä½¿ç”¨å‰5ä¸ªæ ·æœ¬æ¼”ç¤º")

        # æŸ¥çœ‹å‰å‡ ä¸ªæ ·æœ¬
        for i, item in enumerate(dataset):
            if i >= 5:
                break
            print(f"æ ·æœ¬{i + 1}: {item['sentence1'][:50]}... | {item['sentence2'][:50]}...")

        return dataset
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return None


def model_inference_direct():
    """ç¤ºä¾‹7ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†"""
    print("\nâš™ï¸  7. ç›´æ¥æ¨¡å‹æ¨ç†")

    try:
        # ç›´æ¥åŠ è½½æ¨¡å‹
        model = Model.from_pretrained(
            'damo/nlp_structbert_sentiment-classification_chinese-base'
        )

        # åŠ è½½å¯¹åº”çš„é¢„å¤„ç†å™¨
        from modelscope.preprocessors.nlp import TextClassificationTransformersPreprocessor
        preprocessor = TextClassificationTransformersPreprocessor(
            model.model_dir,
            first_sequence='sentence',
            second_sequence=None,
            label='label',
            label2id=model.config.label2id
        )

        print("âœ… æ¨¡å‹å’Œé¢„å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        print(f"æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"æ ‡ç­¾æ˜ å°„: {model.config.label2id}")

        # ç¤ºä¾‹è¾“å…¥
        sample_input = {'sentence': 'è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨'}
        processed = preprocessor(sample_input)
        print(f"é¢„å¤„ç†åçš„è¾“å…¥shape: {processed['input_ids'].shape}")

        return model, preprocessor
    except Exception as e:
        print(f"âŒ ç›´æ¥æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        return None, None


def batch_processing_example():
    """ç¤ºä¾‹8ï¼šæ‰¹é‡å¤„ç†"""
    print("\nğŸ“¦ 8. æ‰¹é‡å¤„ç†ç¤ºä¾‹")

    # åˆ›å»ºpipeline
    batch_pipeline = pipeline(
        task='text-classification',
        model='damo/nlp_structbert_classification_chinese-base'
    )

    # æ‰¹é‡è¾“å…¥
    batch_texts = [
        "è¿™ä¸ªé¤å…çš„é£Ÿç‰©éå¸¸ç¾å‘³",
        "ç‰©æµé€Ÿåº¦å¤ªæ…¢äº†",
        "å®¢æœæ€åº¦å¾ˆå¥½ï¼Œè§£å†³é—®é¢˜å¾ˆå¿«",
        "æ€§ä»·æ¯”ä¸é«˜ï¼Œä¸æ¨èè´­ä¹°"
    ]

    print("æ‰¹é‡å¤„ç†ç»“æœ:")
    results = batch_pipeline(batch_texts)

    for i, (text, result) in enumerate(zip(batch_texts, results)):
        label = "æ­£é¢" if result['label'] == 'positive' else "è´Ÿé¢"
        print(f"{i + 1}. {text[:20]:<20} â†’ {label} ({result['score']:.3f})")


def advanced_features():
    """ç¤ºä¾‹9ï¼šé«˜çº§ç‰¹æ€§æ¼”ç¤º"""
    print("\nğŸš€ 9. é«˜çº§ç‰¹æ€§")

    # 1. æ¨¡å‹ä¿¡æ¯æŸ¥çœ‹
    print("ğŸ“‹ æ¨¡å‹ä¿¡æ¯æŸ¥çœ‹:")
    from modelscope.hub.api import ModelHubAPI
    api = ModelHubAPI()

    # è·å–æ¨¡å‹å¡ç‰‡ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼‰
    print("å¯é€šè¿‡ api.get_model('model_id') è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯")

    # 2. è‡ªå®šä¹‰æ¨ç†å‚æ•°
    print("\nâš™ï¸  è‡ªå®šä¹‰æ¨ç†å‚æ•°:")
    classifier = pipeline(
        task='text-classification',
        model='damo/nlp_structbert_classification_chinese-base'
    )

    # è‡ªå®šä¹‰æ¨ç†å‚æ•°
    custom_config = {
        'max_length': 128,
        'truncation': True,
        'padding': 'max_length'
    }

    result = classifier(
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
        **custom_config
    )
    print(f"ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°æ¨ç†: {result}")

    # 3. Pipelineé…ç½®
    print("\nğŸ”§ Pipelineé…ç½®é€‰é¡¹:")
    print("- device: æŒ‡å®šè¿è¡Œè®¾å¤‡ ('cuda:0', 'cpu')")
    print("- batch_size: æ‰¹é‡å¤§å°")
    print("- model_revision: æ¨¡å‹ç‰ˆæœ¬")
    print("- pipeline_name: æŒ‡å®špipelineç±»å‹")


def main():
    """ä¸»å‡½æ•°"""
    setup_environment()

    # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
    download_model_example()
    nlp_text_classification()
    nlp_sentence_similarity()
    computer_vision_segmentation()
    speech_recognition_example()
    dataset_loading_example()
    model_inference_direct()
    batch_processing_example()
    advanced_features()

    print("\n" + "=" * 60)
    print("ğŸ‰ ModelScope åŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    print("\nğŸ“š æ ¸å¿ƒä½¿ç”¨æ¨¡å¼æ€»ç»“:")
    print("1. Pipelineæ¨¡å¼ (å¿«é€Ÿä¸Šæ‰‹):")
    print("   pipeline = pipeline(task='xxx', model='model_id')")
    print("   result = pipeline(input_data)")
    print("\n2. ç›´æ¥æ¨¡å‹æ¨¡å¼ (çµæ´»æ§åˆ¶):")
    print("   model = Model.from_pretrained('model_id')")
    print("   preprocessor = Preprocessor.from_pretrained('model_id')")
    print("\n3. æ•°æ®é›†æ¨¡å¼ (è®­ç»ƒ/è¯„ä¼°):")
    print("   dataset = MsDataset.load('dataset_name')")
    print("\nğŸ’¡ æ›´å¤šåŠŸèƒ½è¯·å‚è€ƒ: https://modelscope.cn/docs")


if __name__ == '__main__':
    main()